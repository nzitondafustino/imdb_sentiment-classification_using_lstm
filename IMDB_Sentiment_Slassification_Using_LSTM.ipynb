{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download data"
      ],
      "metadata": {
        "id": "9E3lF2XUmycb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "# !tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "cRodVzBHLCh8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import io\n",
        "import string\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence"
      ],
      "metadata": {
        "id": "W-mTHp5xqIZ9"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to get all files"
      ],
      "metadata": {
        "id": "EYbl5WFCm4zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file(dir):\n",
        "  path = []\n",
        "  for par, dirs, files in os.walk(\"aclImdb/\" + dir):\n",
        "    if \"neg\" in par or \"pos\" in par:\n",
        "      path.extend([par + \"/\" + f for f in files])\n",
        "  return path"
      ],
      "metadata": {
        "id": "sY-TXk00rv03"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test files"
      ],
      "metadata": {
        "id": "F6RMlcPgnArX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_files = get_file(\"train\")\n",
        "test_files = get_file(\"test\")"
      ],
      "metadata": {
        "id": "xeQJ9V9ustIO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Vocabulary"
      ],
      "metadata": {
        "id": "aMzkBh06nJC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(file_paths):\n",
        "  for file_path in file_paths:\n",
        "    with io.open(file_path, encoding = 'utf-8') as f:\n",
        "      yield f.read().strip().lower().replace(\"<br />\", \" \").translate(str.maketrans('', '', string.punctuation)).split(\" \")\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_files), specials=[\"<unk>\"],min_freq=10)\n",
        "vocab.set_default_index(0)"
      ],
      "metadata": {
        "id": "r0U7w-gF4Lhy"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dir(vocab.vocab)"
      ],
      "metadata": {
        "id": "PVMtzj_ZwhB7"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset class"
      ],
      "metadata": {
        "id": "vUorwZ36nO7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "\n",
        "  def __init__(self, files) -> None:\n",
        "    self.files = files\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.files)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    path = self.files[index]\n",
        "    label = 1 if \"pos\" in path else 0\n",
        "    with io.open(path, encoding = 'utf-8') as f:\n",
        "      data =  f.read().strip().lower().replace(\"<br />\", \" \").translate(str.maketrans('', '', string.punctuation)).split(\" \")\n",
        "    return torch.LongTensor(vocab.vocab.lookup_indices(data)), label\n",
        "  @staticmethod\n",
        "  def collate_fun(batch):\n",
        "    X = [x for x,_ in batch]\n",
        "    y = [y for _,y in batch]\n",
        "    X = pad_sequence(X,batch_first=True)\n",
        "    return X,torch.LongTensor(y)"
      ],
      "metadata": {
        "id": "e-6QS76q8Of7"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize dataloader"
      ],
      "metadata": {
        "id": "KXIKGfy3nUkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MyDataset(train_files)\n",
        "test_dataset = MyDataset(test_files)\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=64,shuffle=True,collate_fn=MyDataset.collate_fun,num_workers=2)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=64,shuffle=True,collate_fn=MyDataset.collate_fun, num_workers=2)"
      ],
      "metadata": {
        "id": "F7DkBGAP_Kp8"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize model constants"
      ],
      "metadata": {
        "id": "AjYlkvDmnZBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOC_SIZE = len(vocab)\n",
        "EMBED_SIZE = 300\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYER = 2"
      ],
      "metadata": {
        "id": "6lTnhdJdKcsp"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create model"
      ],
      "metadata": {
        "id": "g4MxcVj_nhm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    super(SentimentClassifier,self).__init__()\n",
        "    self.emblayer = nn.Embedding(VOC_SIZE,EMBED_SIZE)\n",
        "    self.lstmlayer = nn.LSTM(EMBED_SIZE, HIDDEN_SIZE,batch_first=True)\n",
        "    self.linear1 = nn.Linear(HIDDEN_SIZE, 32)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.emblayer(x)\n",
        "    x, x_len = self.lstmlayer(x)\n",
        "    x = x[:,-1,:]\n",
        "    x = self.linear1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.linear2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "z3rTz-lT_keI"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getAccuracy(logits,labels):\n",
        "  predictions = torch.argmax(logits,dim=1)\n",
        "  acc = torch.sum(predictions == labels)/predictions.shape[0]\n",
        "  return acc.item()"
      ],
      "metadata": {
        "id": "uzeOsJZSYa4e"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# set seed, initialize model, set criterion and optimizer"
      ],
      "metadata": {
        "id": "rnVeGVjOnnaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model = SentimentClassifier().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "bgsK0h1D_mqs"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Training Loop"
      ],
      "metadata": {
        "id": "xwVUEhwDoCx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 10\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for i in range(epoch):\n",
        "    model.train()\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "    for j, (features,labels) in enumerate(train_dataloader):\n",
        "        t = []\n",
        "        l = []\n",
        "        optimizer.zero_grad()\n",
        "        features = features.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logits = model(features)\n",
        "        # print(getAccuracy(logits,labels))\n",
        "        loss = criterion(logits,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (j+1) % 100 == 0:\n",
        "              print(\"epoch:{}/{}\".format(i+1,epoch,j+1,))\n",
        "        acc = getAccuracy(logits,labels)\n",
        "        train_acc.append(acc)\n",
        "        t.append(loss.item())\n",
        "    av_a_t = sum(train_acc)/len(train_acc)\n",
        "    train_accuracies.append(av_a_t)\n",
        "    av_t = sum(t)/len(t)\n",
        "    print(\"epoch:{}/{},Train loss:{}, Training Accuracy:{}\".format(i+1,epoch,av_t,av_a_t))\n",
        "    train_losses.append(av_t)\n",
        "    model.eval()\n",
        "    for j, (features,labels) in enumerate(test_dataloader):\n",
        "        with torch.no_grad(): \n",
        "            features = features.cuda()\n",
        "            labels = labels.cuda()\n",
        "            logits = model(features)\n",
        "            loss = criterion(logits,labels)\n",
        "            l.append(loss.item())\n",
        "            l.append(loss.item())\n",
        "            acc = getAccuracy(logits,labels)\n",
        "            test_acc.append(acc)\n",
        "    av_a_l = sum(test_acc)/len(test_acc)\n",
        "    av_l = sum(l)/len(l)\n",
        "    print(\"epoch:{}/{},Test loss:{}, Validation Accuracy:{}\".format(i+1,epoch,av_l,av_a_l))\n",
        "    test_accuracies.append(av_a_l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-q1kshoWfQH",
        "outputId": "6352abea-5dab-4f76-de30-2ab5860d8039"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1/10\n",
            "epoch:1/10\n",
            "epoch:1/10\n",
            "epoch:1/10,Train loss:0.6902239322662354, Training Accuracy:0.5028612532883959\n",
            "epoch:1/10,Test loss:0.6933389485949446, Validation Accuracy:0.5011508951101766\n",
            "epoch:2/10\n",
            "epoch:2/10\n",
            "epoch:2/10\n",
            "epoch:2/10,Train loss:0.687137246131897, Training Accuracy:0.5001918158262891\n",
            "epoch:2/10,Test loss:0.69334040974717, Validation Accuracy:0.501798273657289\n",
            "epoch:3/10\n",
            "epoch:3/10\n",
            "epoch:3/10\n",
            "epoch:3/10,Train loss:0.4590366780757904, Training Accuracy:0.6500879155705347\n",
            "epoch:3/10,Test loss:0.4618016934913138, Validation Accuracy:0.81001438615877\n",
            "epoch:4/10\n",
            "epoch:4/10\n",
            "epoch:4/10\n",
            "epoch:4/10,Train loss:0.1909865438938141, Training Accuracy:0.8680786444707904\n",
            "epoch:4/10,Test loss:0.3198232732122511, Validation Accuracy:0.8725063939533575\n",
            "epoch:5/10\n",
            "epoch:5/10\n",
            "epoch:5/10\n",
            "epoch:5/10,Train loss:0.2625444233417511, Training Accuracy:0.9349664323165289\n",
            "epoch:5/10,Test loss:0.3126857839048366, Validation Accuracy:0.8812979540556592\n",
            "epoch:6/10\n",
            "epoch:6/10\n",
            "epoch:6/10\n",
            "epoch:6/10,Train loss:0.11165926605463028, Training Accuracy:0.9708839514676262\n",
            "epoch:6/10,Test loss:0.3637048913656598, Validation Accuracy:0.8769741049203117\n",
            "epoch:7/10\n",
            "epoch:7/10\n",
            "epoch:7/10\n",
            "epoch:7/10,Train loss:0.02229376696050167, Training Accuracy:0.985733695652174\n",
            "epoch:7/10,Test loss:0.4476674150227738, Validation Accuracy:0.8673113811656338\n",
            "epoch:8/10\n",
            "epoch:8/10\n",
            "epoch:8/10\n",
            "epoch:8/10,Train loss:0.007086449768394232, Training Accuracy:0.9910885549872123\n",
            "epoch:8/10,Test loss:0.4828116949981131, Validation Accuracy:0.8744645141579611\n",
            "epoch:9/10\n",
            "epoch:9/10\n",
            "epoch:9/10\n",
            "epoch:9/10,Train loss:0.03393585607409477, Training Accuracy:0.9942215473755546\n",
            "epoch:9/10,Test loss:0.5038404460910642, Validation Accuracy:0.8738650896055314\n",
            "epoch:10/10\n",
            "epoch:10/10\n",
            "epoch:10/10\n",
            "epoch:10/10,Train loss:0.008843466639518738, Training Accuracy:0.9946451406649617\n",
            "epoch:10/10,Test loss:0.5729011488540093, Validation Accuracy:0.876926150925629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hewWsR0zfvsE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}